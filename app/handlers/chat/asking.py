from aiogram import F, Router, flags, html
from aiogram.filters import StateFilter
from aiogram.fsm.context import FSMContext
from aiogram.types import Message, ReplyKeyboardRemove

from clients.openai import OpenAIClient
from config_reader import config
from filters.type_message import TextOrImageOrVoiceFilter
from keyboards.actions import get_exit_keyboard
from middlewares.exceptions import OpenAIExceptionMiddleware
from middlewares.openai_client import OpenAIMiddleware
from states.state import DialogState
from utils.enums import ActionsEnum, MessageTypeEnum
from utils.images import get_image_url
from utils.voice_messages import download_voice_file

asking_router = Router(name=__name__)

asking_router.message.middleware(OpenAIMiddleware(config.api_key.get_secret_value()))
asking_router.message.middleware(OpenAIExceptionMiddleware())


@asking_router.message(F.text == ActionsEnum.ASK)
async def handle_ask_question(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏—è –ø–æ –∫–Ω–æ–ø–∫–µ '–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å'"""

    await message.answer(
        text='–û—Ç–ª–∏—á–Ω–æ! –ù–∞–ø–∏—à–∏ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å, –∏ —è –æ—Ç–ø—Ä–∞–≤–ª—é –µ–≥–æ ChatGPT',
        reply_markup=ReplyKeyboardRemove(),
    )
    await state.set_state(DialogState.active)


@asking_router.message(F.text == ActionsEnum.EXIT)
async def handle_handle_exit(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏—è –ø–æ –∫–Ω–æ–ø–∫–µ '–í—ã–π—Ç–∏'"""

    await state.clear()
    await message.answer(
        text='–î–æ –≤—Å—Ç—Ä–µ—á–∏ üëã',
        reply_markup=ReplyKeyboardRemove(),
    )


@asking_router.message(
    StateFilter(DialogState.active),
    TextOrImageOrVoiceFilter(),
)
@flags.chat_action('typing')
async def handle_question_input(
    message: Message,
    state: FSMContext,
    openai_client: OpenAIClient,
):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""

    await message.answer(text='–û—Ç–ø—Ä–∞–≤–∏–ª —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å, –∂–¥–µ–º –æ—Ç–≤–µ—Ç ‚åõ')

    if message.text:
        answer = await process_text_message(message, openai_client, state)
    elif message.photo:
        answer = await process_image_message(message, openai_client, state)
    else:
        answer = await process_voice_message(message, openai_client, state)

    await state.update_data(last_response=answer)
    await message.reply(text=answer)

    await message.answer(
        text=f'–ú–æ–∂–µ—à—å –∑–∞–¥–∞—Ç—å –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –Ω–∞–∂–∞—Ç—å {html.bold("–í—ã–π—Ç–∏")}, —á—Ç–æ–±—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å –¥–∏–∞–ª–æ–≥',
        reply_markup=get_exit_keyboard(),
    )


async def process_text_message(
    message: Message,
    openai_client: OpenAIClient,
    state: FSMContext,
):
    return await openai_client.ask(
        user_text=message.text,
        state=state,
        type_message=MessageTypeEnum.TEXT,
    )


async def process_image_message(
    message: Message,
    openai_client: OpenAIClient,
    state: FSMContext,
):
    image_url = await get_image_url(message)
    return await openai_client.ask(
        image_url=image_url,
        type_message=MessageTypeEnum.IMAGE_URL,
        state=state,
    )


async def process_voice_message(
    message: Message,
    openai_client: OpenAIClient,
    state: FSMContext,
):
    voice_file_path = await download_voice_file(message)
    transcription_text = await openai_client.convert_voice_to_text(voice_file_path)
    answer = await openai_client.ask(
        state=state,
        user_text=transcription_text,
        type_message=MessageTypeEnum.TEXT,
    )
    return answer
